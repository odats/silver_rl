{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Elementary Reinforcement Learning from RL Course by David Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odats/openai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "s0 = env.reset()\n",
    "env.render()\n",
    "# when you make action there is 33% chance to get to another space\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(q_table, state, env, epsilon = 0.25):\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()     \n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "    return action\n",
    "\n",
    "def e_greedy_policy_decay(q_table, state, k, number_of_runs, env):\n",
    "    epsilon = 1 - (k/number_of_runs)\n",
    "    \n",
    "    actions_count = env.action_space.n\n",
    "    act_greedy = epsilon / actions_count + (1 - epsilon)\n",
    "    \n",
    "    if random.random() < act_greedy:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "\n",
    "def ucb_policy(q_table, state, q_table_n, k, number_of_runs, env):\n",
    "    action = np.argmax(np.add(q_table[state,:], 2*(np.log(k) / q_table_n[state,:])))\n",
    "    \n",
    "    return action\n",
    "\n",
    "def random_policy(q_table, state, env):\n",
    "    action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def greedy_policy(q_table, state):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(item, size=1):\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[item]=1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, q_table, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    state_values = np.zeros(env.observation_space.n)\n",
    "    n_of_s = np.zeros(env.observation_space.n)\n",
    "\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    #q_table = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "    q_table_n = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # fill action states values\n",
    "        q_table_n_first = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            \n",
    "            # MC first occurrence\n",
    "            if MC_first_occurrence:\n",
    "                if q_table_n_first[step_state][step_action] != 0:\n",
    "                    continue\n",
    "                q_table_n_first[step_state][step_action] = 1\n",
    "            \n",
    "            q_table_n[step_state][step_action] += 1\n",
    "            n = q_table_n[step_state][step_action]\n",
    "\n",
    "            current_value = q_table[step_state][step_action]\n",
    "            alfa = (1/n)\n",
    "            new_value = current_value + alfa * (trajectory_total_discounted_rewards[step] - current_value)\n",
    "            q_table[step_state][step_action] = new_value\n",
    "\n",
    "        # fill states values\n",
    "        for step, state in enumerate(trajectory_states):\n",
    "            n_of_s[state] += 1 \n",
    "            state_values[state] = state_values[state] + (1/n_of_s[state]) * (trajectory_total_discounted_rewards[step]-state_values[state])\n",
    "            \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table, new_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table, new_state, env)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][new_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning. SARSA off model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "                    \n",
    "            target_action = greedy_policy(q_table, new_state)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][target_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vf_tf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    tf.reset_default_graph()\n",
    "    #These lines establish the feed-forward part of the network used to choose actions\n",
    "    input_state = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "    predicted_q_values = tf.matmul(input_state, W)\n",
    "    predicted_action = tf.argmax(predicted_q_values, 1)\n",
    "\n",
    "    #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "    real_q_values = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(real_q_values - predicted_q_values))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    update_model = trainer.minimize(loss)\n",
    "    \n",
    "    # start\n",
    "    init = tf.global_variables_initializer()\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for k in tqdm(range(1, number_of_runs)):\n",
    "            trajectory_states = []\n",
    "            trajectory_actions = []\n",
    "            trajectory_rewards = []\n",
    "            trajectory_total_discounted_rewards = []\n",
    "\n",
    "            current_state = env.reset()\n",
    "            done = False\n",
    "            while not done: \n",
    "                a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "                action = a[0]\n",
    "                if use_e_greedy_policy_decay:\n",
    "                    action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "                    #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "                else:\n",
    "                    action = e_greedy_policy(q_table, 0, env)\n",
    "\n",
    "                new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "                trajectory_states.append(current_state)\n",
    "                trajectory_actions.append(action)\n",
    "                trajectory_rewards.append(reward)\n",
    "\n",
    "                current_state = new_state\n",
    "\n",
    "            for idx_state, state in enumerate(trajectory_states):\n",
    "                total_discounted_reward = 0\n",
    "                for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                    total_discounted_reward += reward * (gama**idx_reward)\n",
    "                trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "\n",
    "            # learning\n",
    "            # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "            for step in range(len(trajectory_states)):\n",
    "                step_state = trajectory_states[step]\n",
    "                step_action = trajectory_actions[step]\n",
    "                Gt = trajectory_total_discounted_rewards[step]\n",
    "                estimated_q_table = sess.run(predicted_q_values, \n",
    "                                             feed_dict={input_state:[one_hot_encoding(step_state, 16)]})\n",
    "                # make stimated values real\n",
    "                estimated_q_table[0, step_action] = Gt\n",
    "\n",
    "                #Train our network using target and predicted Q values\n",
    "                _,W1 = sess.run([update_model, W],\n",
    "                                feed_dict={input_state:[one_hot_encoding(step_state, 16)], real_q_values:estimated_q_table})\n",
    "\n",
    "            # trace log   \n",
    "            if reward == 1:\n",
    "                won_count += 1\n",
    "            if use_trace_of_learning and k%1000 == 0:\n",
    "                trace_of_won.append(won_count)\n",
    "                won_count = 0\n",
    "        \n",
    "        q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for current_state in range(16):\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "                \n",
    "            q_table_final[current_state] = q_table\n",
    "        print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vf_tf_sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    tf.reset_default_graph()\n",
    "    #These lines establish the feed-forward part of the network used to choose actions\n",
    "    input_state = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "    predicted_q_values = tf.matmul(input_state, W)\n",
    "    predicted_action = tf.argmax(predicted_q_values, 1)\n",
    "\n",
    "    #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "    real_q_values = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(real_q_values - predicted_q_values))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    update_model = trainer.minimize(loss)\n",
    "    \n",
    "    # start\n",
    "    init = tf.global_variables_initializer()\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for k in tqdm(range(1, number_of_runs)):\n",
    "            current_state = env.reset()\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "            current_action = a[0]\n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, 0, env)\n",
    "\n",
    "            done = False\n",
    "            while not done: \n",
    "                a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "\n",
    "                new_state, reward, done, prob = env.step(current_action)\n",
    "\n",
    "                a, q_table_new = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(new_state, 16)]})\n",
    "\n",
    "                if use_e_greedy_policy_decay:\n",
    "                    new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "                else:\n",
    "                    new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "\n",
    "\n",
    "                # make stimated values real\n",
    "                q_table[0, current_action] = reward + gama* q_table_new[0][new_action]\n",
    "\n",
    "                _,W1 = sess.run([update_model, W],\n",
    "                                    feed_dict={input_state:[one_hot_encoding(current_state, 16)], real_q_values:q_table})\n",
    "\n",
    "                current_state = new_state\n",
    "                current_action = new_action\n",
    "\n",
    "            # trace log   \n",
    "            if reward == 1:\n",
    "                won_count += 1\n",
    "            if use_trace_of_learning and k%1000 == 0:\n",
    "                trace_of_won.append(won_count)\n",
    "                won_count = 0\n",
    "        \n",
    "        q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for current_state in range(16):\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "\n",
    "            q_table_final[current_state] = q_table\n",
    "        print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based on numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_table_approximation(W, state, env):\n",
    "    # forward propagetion\n",
    "    qa1 = np.dot(W, prepare_x(state, 0))\n",
    "    qa2 = np.dot(W, prepare_x(state, 1))\n",
    "    qa3 = np.dot(W, prepare_x(state, 2))\n",
    "    qa4 = np.dot(W, prepare_x(state, 3))\n",
    "    \n",
    "    q_table = np.array([[qa1, qa2, qa3, qa4]])\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def evaluate_nn_policy(env, W, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            q_table = q_table_approximation(W, state, env)\n",
    "            action = greedy_policy(q_table, 0)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes\n",
    "\n",
    "def prepare_x(state, action):\n",
    "    one_hot_state = one_hot_encoding(state, 16)\n",
    "    one_hot_action = one_hot_encoding(action, 4)\n",
    "    X = np.hstack((one_hot_state, one_hot_action))\n",
    "    \n",
    "    return X\n",
    "\n",
    "def vf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # learning\n",
    "        # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            Gt = trajectory_total_discounted_rewards[step]\n",
    "            q_table = q_table_approximation(W, step_state, env)\n",
    "            estimated_Gt = q_table[0][step_action]\n",
    "            \n",
    "            error = Gt - estimated_Gt\n",
    "            X = prepare_x(step_state, step_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "    \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won\n",
    "    \n",
    "        \n",
    "def semi_gradient_sarsa_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # state, actions\n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    #W = np.random.random(H) / np.sqrt(H)\n",
    "    # W = np.zeros(env.observation_space.n + env.action_space.n)\n",
    "    W_start = np.copy(W)\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        q_table = q_table_approximation(W, current_state, env)\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if done and reward==0:\n",
    "                reward = -10\n",
    "            if done and reward==1:\n",
    "                reward = 10\n",
    "            \n",
    "            if done:\n",
    "                error = (reward - q_table[0][current_action])\n",
    "                X = prepare_x(current_state, current_action)\n",
    "                grad = error * X # backpropagate error\n",
    "                W += learning_rate * grad\n",
    "                continue\n",
    "            \n",
    "            q_table_new = q_table_approximation(W, new_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "            \n",
    "            # learning\n",
    "            error = (reward + gama* q_table_new[0][new_action] - q_table[0][current_action])\n",
    "            X = prepare_x(current_state, current_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 10:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "    \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [31:27<00:00, 52.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.92719138e-01 4.65400040e-01 4.23009396e-01 4.64795649e-01]\n",
      " [2.60544598e-01 1.68261051e-01 2.18960330e-01 5.75836003e-01]\n",
      " [3.13449264e-01 2.94181436e-01 2.79446125e-01 4.87562358e-01]\n",
      " [2.04874650e-01 1.39907122e-01 2.18162209e-01 4.81235147e-01]\n",
      " [6.22616231e-01 4.37918007e-01 3.27309042e-01 3.03729564e-01]\n",
      " [6.44968124e-03 5.95798483e-04 4.87060053e-03 4.48204530e-03]\n",
      " [3.92215967e-01 2.99323816e-02 9.19836387e-02 7.40709156e-02]\n",
      " [7.87636824e-03 4.51071374e-03 5.01943473e-03 5.42083615e-03]\n",
      " [3.32819343e-01 3.88923019e-01 3.80261540e-01 6.69931591e-01]\n",
      " [2.79669225e-01 7.23484993e-01 1.69624701e-01 2.32559681e-01]\n",
      " [6.57936513e-01 3.30214858e-01 2.91209608e-01 2.49934345e-01]\n",
      " [6.46266667e-03 6.06213324e-03 7.65433302e-04 6.36084657e-03]\n",
      " [4.24531708e-03 1.18211983e-03 5.31827332e-03 9.49676987e-03]\n",
      " [4.01750267e-01 6.28881574e-01 7.89272308e-01 5.28264761e-01]\n",
      " [6.79492414e-01 9.13728833e-01 6.96418643e-01 7.26852596e-01]\n",
      " [9.03613213e-03 1.65737025e-03 8.76726117e-03 2.26657023e-03]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113862908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAGFRJREFUeJzt3Xu0nXV95/H3R6LxWkNAIxA0gDgOOFY7Z8BLO+uM3LVcluIUtRA7Wqa1rrE6dsSlSyiFFpmpto62NUU0Qi0qLTQtOAjoaa21ykVcQqsm3JpwFQhIwHDzO3/sJ7o5nCQ7Ob99dnbyfq2113kuv/083985yf6c3/N79j6pKiRJmq0njboASdL2wUCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKtI1J8uokK5OsS3LMqOsZVJJTkpzbLT+/q3+nUdeluWOgaKSSTCVZm2R+37aTkvzDDG13TfJwkpd067sl+fMkt3YvXjck+UySFzes76YkB7c63oBOBT5eVc+sqgvn+Nwb+vzj7nt6R/c9feaWHKOq/q2r/7Fh1altj4GikUmyBPgloICj+nadC7wqyV7TnnIc8N2qujbJLsA/AU/vjvEs4BeAvwcOGfD882ZT/xDP8QLguo0cL0nm4v/tkVX1THrf0wngg3NwTo05A0WjdALwz8BngKUbNlbVGuArwPEztP9st/xu4EfA8VV1ffXcW1Wfrqr/O9PJkkwmWZPkfUluBz7dbf/lJNckuTfJPyV5abf9HOD5wN92v63/rw3HmHbcn45iuss+5yc5N8mPgLd2276Q5LNJ7k9yXZKJjdR4PbB33znnd6O405N8HXgQ2DvJ7klWJLknyaokv953jFOSfLGr4f4k303yoiTvT3JnktVJDt3Mz2bDz+IW4EvAhlHhRs87rR9LktSGQE2yMMmnu9Hk2iQXdtuvTXJk3/OenOSuJC8fpD5tWwwUjdIJwF90j8OSLOrbt5y+QEny74CXAZ/rNh0MXFBVP9nCcz4PWEhvFHBi98J1NvDfgV2ATwIrksyvquOBf6P7bb2qzhzwHEcD5wMLur5BbwR2XrdtBfDxmZ5YVftMO+dD3a7jgRPpjcRu7o61BtgdOBb4/SSv6TvUkcA5wM7At4FL6P1/34PeJbVPDtKRJHsCr+2OwQDn3Zhz6I0m9weeC3y02/5Z4Ff72r0WuK2qvo3GjoGikUjyi/Re1L9QVVcB1wNv7mtyAbAoyau69ROAL1XVD7v1XYHb+453VDfCuD/Jlzdx6p8AJ1fVQ1X1Y3ov0p+sqm9W1WNVtRx4CHjFLLr3jaq6sKp+0p0D4B+r6uJuTuEc4Oe38JifqarrqupReqH4auB9VbW+qq4BzqL3Pdrga1V1Sdf+i8BzgDOq6hF6obAkyYJNnO/CJPcC/0jvMuLvd+GyufM+QZLdgCOA36iqtVX1SFX9fbf7XOC1SX6uWz+e3vdHY8hA0agsBb5cVXd165/j8Ze9HqT3QnhCkgBv4WeXuwDuBnbra7+iqhbQuxT2lE2c94dVtb5v/QXA/+zC6N7uRXRPer+Bb63VM2y7vW/5QeCpWzi/0n/M3YF7qur+vm030xt9bHBH3/KPgbv6Jsg3hNymJtqPqaoFVfWCqnpHF4yDnHcme3bPWzt9R1XdCnwdeEMXcEfws1GdxszQJyWl6ZI8DfivwE7dXAbAfGBBkp+vqu9025YDFwJ/Te9Sz9/2HeZy4Jgkv7uFl72mf7z2auD0qjp9wPYP0Lt0s6EvO9H77X9Tz2mh/5i3AguTPKvvxf35wC1DOG+/rT3v6u55C6rq3hn2LwfeTu/16BvdvI3GkCMUjcIxwGPAfvTmRV4G/Hvga0y7bAPcCywDzquqh/v2fYTe/MA5Sfbp7n56VnesLfHnwG8kObA7xjOSvK47FvR+09+7r/0P6I0uXpfkyfTufprPHKqq1fTucPuDJE/tbiJ4G73LR9vceavqNnoT+3+SZOdu4v0/9zW5kN7dZO/i8aNQjRkDRaOwFPh0916F2zc86E1Uv2XDpaDq/bGez9K7LPW4F5ruUtkrgPX0rvPfD1xDbyTzm4MWUlVXAr/enXstsAp4a1+TPwA+2F0Oe29V3Qe8g97cwS30RiyPu+trjrwJWEJv1HABvXmhy7bh8x4PPAJ8D7gT+O0NO7rLaX8F7EVvNKoxFf/AlqRRS/Ih4EVV9aubbaxtlnMokkYqyUJ6l86mv+9IY8ZLXpJGpntj5Gp6t4Q/4eN2NF685CVJasIRiiSpiR1qDmXXXXetJUuWjLqMLfLAAw/wjGc8Y9RlzCn7vGOwz+Pjqquuuquqpr/f6gl2qEBZsmQJV1555ajL2CJTU1NMTk6Ouow5ZZ93DPZ5fCS5eZB2XvKSJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDUx0kBJcniS7ydZleSkGfbPT/L5bv83kyyZtv/5SdYlee9c1SxJmtnIAiXJTsAngCOA/YA3JdlvWrO3AWur6oXAR4EPT9v/EeBLw65VkrR5oxyhHACsqqobquph4Dzg6GltjgaWd8vnAwclCUCSY4AbgevmqF5J0ibMG+G59wBW962vAQ7cWJuqejTJfcAuSdYD7wMOATZ5uSvJicCJAIsWLWJqaqpJ8XNl3bp1Y1fzbNnnHYN93v6MMlBm4xTgo1W1rhuwbFRVLQOWAUxMTNTk5OTQi2tpamqKcat5tuzzjsE+b39GGSi3AHv2rS/uts3UZk2SecCzgbvpjWSOTXImsAD4SZL1VfXx4ZctSZrJKAPlCmDfJHvRC47jgDdPa7MCWAp8AzgW+EpVFfBLGxokOQVYZ5hI0miNLFC6OZF3ApcAOwFnV9V1SU4FrqyqFcCngHOSrALuoRc6kqRt0EjnUKrqYuDiads+1Le8HnjjZo5xylCKkyRtEd8pL0lqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEyMNlCSHJ/l+klVJTpph//wkn+/2fzPJkm77IUmuSvLd7utr5rp2SdLjjSxQkuwEfAI4AtgPeFOS/aY1exuwtqpeCHwU+HC3/S7gyKr6D8BS4Jy5qVqStDGjHKEcAKyqqhuq6mHgPODoaW2OBpZ3y+cDByVJVX27qm7ttl8HPC3J/DmpWpI0o1EGyh7A6r71Nd22GdtU1aPAfcAu09q8Abi6qh4aUp2SpAHMG3UBs5Fkf3qXwQ7dRJsTgRMBFi1axNTU1NwU18i6devGrubZss87Bvu8/RlloNwC7Nm3vrjbNlObNUnmAc8G7gZIshi4ADihqq7f2EmqahmwDGBiYqImJydb1T8npqamGLeaZ8s+7xjs8/ZnlJe8rgD2TbJXkqcAxwErprVZQW/SHeBY4CtVVUkWABcBJ1XV1+esYknSRo0sULo5kXcClwD/Cnyhqq5LcmqSo7pmnwJ2SbIKeA+w4dbidwIvBD6U5Jru8dw57oIkqc9I51Cq6mLg4mnbPtS3vB544wzPOw04begFSpIG5jvlJUlNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKmJgQIlyTOSPKlbflGSo5I8ebilSZLGyaAjlH8AnppkD+DLwPHAZ4ZVlCRp/AwaKKmqB4HXA39SVW8E9h9eWZKkcTNwoCR5JfAW4KJu207DKUmSNI4GDZR3Ae8HLqiq65LsDXx1eGVJksbNvAHbLaqqozasVNUNSb42pJokSWNo0BHK+wfcJknaQW1yhJLkCOC1wB5JPta36+eAR4dZmCRpvGzuktetwFXAUd3XDe4H3j2soiRJ42eTgVJV3wG+k+TcqnJEIknaqM1d8vouUN3yE/ZX1UuHU5Ykadxs7pLXLw/z5EkOB/6Y3ntazqqqM6btnw98FviPwN3Ar1TVTd2+9wNvAx4D/kdVXTLMWiVJm7a5S143D+vESXYCPgEcAqwBrkiyoqr+pa/Z24C1VfXCJMcBHwZ+Jcl+wHH03q2/O3BZkhdV1WPDqleStGmDfjjk/Ul+1D3WJ3ksyY9mee4DgFVVdUNVPQycBxw9rc3RwPJu+XzgoPSuvR0NnFdVD1XVjcCq7niSpBEZ6I2NVfWsDct9L+ivmOW59wBW962vAQ7cWJuqejTJfcAu3fZ/nvbcPWY6SZITgRMBFi1axNTU1CzLnlvr1q0bu5pnyz7vGOzz9mfQd8r/VFUVcGGSk4GT2pfUVlUtA5YBTExM1OTk5GgL2kJTU1OMW82zZZ93DPZ5+zNQoCR5fd/qk4AJYP0sz30LsGff+uJu20xt1iSZBzyb3uT8IM+VJM2hQUcoR/YtPwrcxBPnO7bUFcC+SfaiFwbHAW+e1mYFsBT4BnAs8JWqqiQrgM8l+Qi9Sfl9gW/Nsh5J0iwMOofya61P3M2JvBO4hN5tw2d3n2R8KnBlVa0APgWck2QVcA+90KFr9wXgX+gF3G95h5ckjdagl7zOBE4Dfgz8P+ClwLur6tzZnLyqLgYunrbtQ33L64E3buS5pwOnz+b8kqR2Bv204UOr6kf03uh4E/BC4HeGVZQkafwMGigbRjKvA75YVfcNqR5J0pgadFL+75J8j94lr99M8hxmf5eXJGk7MtAIpapOAl4FTFTVI8CDzP4uL0nSdmTQj155OvAO4E+7TbvTey+KJEnA4HMonwYepjdKgd77Rk4bSkWSpLE0aKDsU1VnAo8AVNWDwBP/QIokaYc1aKA8nORp/OyPbe0DPDS0qiRJY2ezd3l1ny78Z/Te0Lhnkr8AXg28dbilSZLGyWYDpfvsrN8BJul9ZH2Ad1XVXUOuTZI0RgZ9H8rVwN5VddEwi5Ekja9BA+VA4C1JbgYeoDdKqap66dAqkySNlUED5bChViFJGnuDfnz9zcMuRJI03ga9bViSpE0yUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1MZJASbIwyaVJVnZfd95Iu6Vdm5VJlnbbnp7koiTfS3JdkjPmtnpJ0kxGNUI5Cbi8qvYFLu/WHyfJQuBken/P/gDg5L7g+T9V9WLg5cCrkxwxN2VLkjZmVIFyNLC8W14OHDNDm8OAS6vqnqpaC1wKHF5VD1bVVwGq6mHgamDxHNQsSdqEUQXKoqq6rVu+HVg0Q5s9gNV962u6bT+VZAFwJL1RjiRphOYN68BJLgOeN8OuD/SvVFUlqa04/jzgL4GPVdUNm2h3InAiwKJFi5iamtrSU43UunXrxq7m2bLPOwb7vP0ZWqBU1cEb25fkjiS7VdVtSXYD7pyh2S3AZN/6YmCqb30ZsLKq/mgzdSzr2jIxMVGTk5Obar7NmZqaYtxqni37vGOwz9ufUV3yWgEs7ZaXAn8zQ5tLgEOT7NxNxh/abSPJacCzgd+eg1olSQMYVaCcARySZCVwcLdOkokkZwFU1T3A7wFXdI9Tq+qeJIvpXTbbD7g6yTVJ3j6KTkiSfmZol7w2paruBg6aYfuVwNv71s8Gzp7WZg2QYdcoSdoyvlNektSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMjCZQkC5NcmmRl93XnjbRb2rVZmWTpDPtXJLl2+BVLkjZnVCOUk4DLq2pf4PJu/XGSLAROBg4EDgBO7g+eJK8H1s1NuZKkzRlVoBwNLO+WlwPHzNDmMODSqrqnqtYClwKHAyR5JvAe4LQ5qFWSNIB5Izrvoqq6rVu+HVg0Q5s9gNV962u6bQC/B/wh8ODmTpTkROBEgEWLFjE1NbWVJY/GunXrxq7m2bLPOwb7vP0ZWqAkuQx43gy7PtC/UlWVpLbguC8D9qmqdydZsrn2VbUMWAYwMTFRk5OTg55qmzA1NcW41Txb9nnHYJ+3P0MLlKo6eGP7ktyRZLequi3JbsCdMzS7BZjsW18MTAGvBCaS3ESv/ucmmaqqSSRJIzOqOZQVwIa7tpYCfzNDm0uAQ5Ps3E3GHwpcUlV/WlW7V9US4BeBHxgmkjR6owqUM4BDkqwEDu7WSTKR5CyAqrqH3lzJFd3j1G6bJGkbNJJJ+aq6Gzhohu1XAm/vWz8bOHsTx7kJeMkQSpQkbSHfKS9JasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktREqmrUNcyZJD8Ebh51HVtoV+CuURcxx+zzjsE+j48XVNVzNtdohwqUcZTkyqqaGHUdc8k+7xjs8/bHS16SpCYMFElSEwbKtm/ZqAsYAfu8Y7DP2xnnUCRJTThCkSQ1YaBIkpowULYBSRYmuTTJyu7rzhtpt7RrszLJ0hn2r0hy7fArnr3Z9DnJ05NclOR7Sa5LcsbcVr9lkhye5PtJViU5aYb985N8vtv/zSRL+va9v9v+/SSHzWXds7G1fU5ySJKrkny3+/qaua59a8zmZ9ztf36SdUneO1c1D0VV+RjxAzgTOKlbPgn48AxtFgI3dF937pZ37tv/euBzwLWj7s+w+ww8HfgvXZunAF8Djhh1nzbSz52A64G9u1q/A+w3rc07gD/rlo8DPt8t79e1nw/s1R1np1H3ach9fjmwe7f8EuCWUfdnmP3t238+8EXgvaPuz2wejlC2DUcDy7vl5cAxM7Q5DLi0qu6pqrXApcDhAEmeCbwHOG0Oam1lq/tcVQ9W1VcBquph4Gpg8RzUvDUOAFZV1Q1drefR63u//u/F+cBBSdJtP6+qHqqqG4FV3fG2dVvd56r6dlXd2m2/DnhakvlzUvXWm83PmCTHADfS6+9YM1C2DYuq6rZu+XZg0Qxt9gBW962v6bYB/B7wh8CDQ6uwvdn2GYAkC4AjgcuHUWQDm+1Df5uqehS4D9hlwOdui2bT535vAK6uqoeGVGcrW93f7pfB9wG/Owd1Dt28URewo0hyGfC8GXZ9oH+lqirJwPdyJ3kZsE9VvXv6ddlRG1af+44/D/hL4GNVdcPWValtUZL9gQ8Dh466liE7BfhoVa3rBixjzUCZI1V18Mb2JbkjyW5VdVuS3YA7Z2h2CzDZt74YmAJeCUwkuYnez/O5SaaqapIRG2KfN1gGrKyqP2pQ7rDcAuzZt7642zZTmzVdSD4buHvA526LZtNnkiwGLgBOqKrrh1/urM2mvwcCxyY5E1gA/CTJ+qr6+PDLHoJRT+L4KID/zeMnqM+coc1CetdZd+4eNwILp7VZwvhMys+qz/Tmi/4KeNKo+7KZfs6jdzPBXvxswnb/aW1+i8dP2H6hW96fx0/K38B4TMrPps8LuvavH3U/5qK/09qcwphPyo+8AB8FvWvHlwMrgcv6XjQngLP62v03ehOzq4Bfm+E44xQoW91ner8BFvCvwDXd4+2j7tMm+vpa4Af07gT6QLftVOCobvmp9O7wWQV8C9i777kf6J73fbbRO9la9hn4IPBA38/1GuC5o+7PMH/GfccY+0Dxo1ckSU14l5ckqQkDRZLUhIEiSWrCQJEkNWGgSJKaMFCkOZJkQZJ3jLoOaVgMFGnuLKD3qbOP071zWhp7/kOW5s4ZwD5JrgEeAdYDa4EXJzkPuKe6j5FJcjpwZ1X98ciqlbaQb2yU5kj34Z1/V1UvSTIJXAS8pKpu7Pb9dVX9QpIn0fsEgQOq6u4RlSttMUco0uh8q3p/54SquinJ3UleTu+j/L9tmGjcGCjS6Dwwbf0s4K30PvL/7DmvRpolJ+WluXM/8KxN7L+A3l/h/E/AJXNSkdSQIxRpjlTV3Um+nuRa4MfAHdP2P5zkq8C9VfXYSIqUZsFJeWkb0U3GXw28sapWjroeaUt5yUvaBiTZj97fyrjcMNG4coQiSWrCEYokqQkDRZLUhIEiSWrCQJEkNWGgSJKa+P9mQM0i0EclAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136fde80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 0.0\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "y = []\n",
    "number_of_try = 1\n",
    "number_of_runs = 100000\n",
    "q_tables = []\n",
    "\n",
    "for i in range(number_of_try):\n",
    "    # train\n",
    "#     W, trace_of_learning, trace_of_won = semi_gradient_sarsa_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                       use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "\n",
    "    \n",
    "    W, trace_of_learning, trace_of_won = vf_tf_sarsa_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "                                                                use_e_greedy_policy_decay=True)\n",
    "    \n",
    "#     W, trace_of_learning, trace_of_won = vf_tf_monte_carlo_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "#                                                                 use_e_greedy_policy_decay=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "    result = 0\n",
    "    q_table = []\n",
    "    \n",
    "#     q_table, trace_of_learning, trace_of_won = sarsa_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "\n",
    "#     q_table, trace_of_learning, trace_of_won = q_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=False, use_trace_of_learning=True)\n",
    "    sleep(0.1)\n",
    "    # evaluate\n",
    "    #result = evaluate_policy(env, q_table, 100)\n",
    "\n",
    "    # data for charts\n",
    "    x.append(i)\n",
    "    y.append(result)\n",
    "    q_tables.append(q_table)\n",
    "\n",
    "    # show learning\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(trace_of_learning)\n",
    "    plt.title('Learning process')\n",
    "    plt.ylabel('Results')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(trace_of_won)\n",
    "    plt.xlabel('tries (each step is 1000)')\n",
    "    plt.ylabel('Won games over time')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# total analitics    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set(xlabel='try', ylabel='results', title='AVG return from Policy')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(str(number_of_runs) + '_' + str(number_of_try) + '_plot.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean', np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_tables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mclean_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mclean_q_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36mget_clean_q_table\u001b[0;34m(q_table)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_clean_q_table(q_table):\n",
    "    clean = np.zeros(q_table.shape)\n",
    "    for state in range(q_table.shape[0]):\n",
    "        max_action = np.argmax(q_table[state,:])\n",
    "        clean[state][max_action] = 1\n",
    "    return clean\n",
    "\n",
    "clean_q_table = [] \n",
    "total_sum = np.zeros((16,4))\n",
    "for q_table in q_tables:\n",
    "    clean_table = get_clean_q_table(q_table)\n",
    "    clean_q_table.append(clean_table)\n",
    "    total_sum = np.add(total_sum, clean_table)\n",
    "    \n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# for n in range(number_of_try):\n",
    "#     xs = np.arange(16*4)\n",
    "#     ys = clean_q_table[n].flatten()\n",
    "#     ax.bar(xs, ys, zs=n, zdir='y')\n",
    "# ax.set_xlabel('state-action')\n",
    "# ax.set_ylabel('tries')\n",
    "# ax.set_zlabel('value: 0 or 1')\n",
    "# plt.show()\n",
    "\n",
    "print(total_sum/number_of_try)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
