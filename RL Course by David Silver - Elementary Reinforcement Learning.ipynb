{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Elementary Reinforcement Learning from RL Course by David Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odats/openai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "s0 = env.reset()\n",
    "env.render()\n",
    "# when you make action there is 33% chance to get to another space\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(q_table, state, env, epsilon = 0.25):\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()     \n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "    return action\n",
    "\n",
    "def e_greedy_policy_decay(q_table, state, k, number_of_runs, env):\n",
    "    epsilon = 1 - (k/number_of_runs)\n",
    "    \n",
    "    actions_count = env.action_space.n\n",
    "    act_greedy = epsilon / actions_count + (1 - epsilon)\n",
    "    \n",
    "    if random.random() < act_greedy:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "\n",
    "def ucb_policy(q_table, state, q_table_n, k, number_of_runs, env):\n",
    "    action = np.argmax(np.add(q_table[state,:], 2*(np.log(k) / q_table_n[state,:])))\n",
    "    \n",
    "    return action\n",
    "\n",
    "def random_policy(q_table, state, env):\n",
    "    action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def greedy_policy(q_table, state):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(item, size=1):\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[item]=1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, q_table, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    state_values = np.zeros(env.observation_space.n)\n",
    "    n_of_s = np.zeros(env.observation_space.n)\n",
    "\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    #q_table = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "    q_table_n = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # fill action states values\n",
    "        q_table_n_first = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            \n",
    "            # MC first occurrence\n",
    "            if MC_first_occurrence:\n",
    "                if q_table_n_first[step_state][step_action] != 0:\n",
    "                    continue\n",
    "                q_table_n_first[step_state][step_action] = 1\n",
    "            \n",
    "            q_table_n[step_state][step_action] += 1\n",
    "            n = q_table_n[step_state][step_action]\n",
    "\n",
    "            current_value = q_table[step_state][step_action]\n",
    "            alfa = (1/n)\n",
    "            new_value = current_value + alfa * (trajectory_total_discounted_rewards[step] - current_value)\n",
    "            q_table[step_state][step_action] = new_value\n",
    "\n",
    "        # fill states values\n",
    "        for step, state in enumerate(trajectory_states):\n",
    "            n_of_s[state] += 1 \n",
    "            state_values[state] = state_values[state] + (1/n_of_s[state]) * (trajectory_total_discounted_rewards[step]-state_values[state])\n",
    "            \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table, new_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table, new_state, env)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][new_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning. SARSA off model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "                    \n",
    "            target_action = greedy_policy(q_table, new_state)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][target_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vf_tf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    tf.reset_default_graph()\n",
    "    #These lines establish the feed-forward part of the network used to choose actions\n",
    "    input_state = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "    predicted_q_values = tf.matmul(input_state, W)\n",
    "    predicted_action = tf.argmax(predicted_q_values, 1)\n",
    "\n",
    "    #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "    real_q_values = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(real_q_values - predicted_q_values))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    update_model = trainer.minimize(loss)\n",
    "    \n",
    "    # start\n",
    "    init = tf.global_variables_initializer()\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for k in tqdm(range(1, number_of_runs)):\n",
    "            trajectory_states = []\n",
    "            trajectory_actions = []\n",
    "            trajectory_rewards = []\n",
    "            trajectory_total_discounted_rewards = []\n",
    "\n",
    "            current_state = env.reset()\n",
    "            done = False\n",
    "            while not done: \n",
    "                a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "                action = a[0]\n",
    "                if use_e_greedy_policy_decay:\n",
    "                    action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "                    #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "                else:\n",
    "                    action = e_greedy_policy(q_table, 0, env)\n",
    "\n",
    "                new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "                trajectory_states.append(current_state)\n",
    "                trajectory_actions.append(action)\n",
    "                trajectory_rewards.append(reward)\n",
    "\n",
    "                current_state = new_state\n",
    "\n",
    "            for idx_state, state in enumerate(trajectory_states):\n",
    "                total_discounted_reward = 0\n",
    "                for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                    total_discounted_reward += reward * (gama**idx_reward)\n",
    "                trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "\n",
    "            # learning\n",
    "            # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "            for step in range(len(trajectory_states)):\n",
    "                step_state = trajectory_states[step]\n",
    "                step_action = trajectory_actions[step]\n",
    "                Gt = trajectory_total_discounted_rewards[step]\n",
    "                estimated_q_table = sess.run(predicted_q_values, \n",
    "                                             feed_dict={input_state:[one_hot_encoding(step_state, 16)]})\n",
    "                # make stimated values real\n",
    "                estimated_q_table[0, step_action] = Gt\n",
    "\n",
    "                #Train our network using target and predicted Q values\n",
    "                _,W1 = sess.run([update_model, W],\n",
    "                                feed_dict={input_state:[one_hot_encoding(step_state, 16)], real_q_values:estimated_q_table})\n",
    "\n",
    "            # trace log   \n",
    "            if reward == 1:\n",
    "                won_count += 1\n",
    "            if use_trace_of_learning and k%1000 == 0:\n",
    "                trace_of_won.append(won_count)\n",
    "                won_count = 0\n",
    "        \n",
    "        q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for current_state in range(16):\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "                \n",
    "            q_table_final[current_state] = q_table\n",
    "        print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vf_tf_sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    tf.reset_default_graph()\n",
    "    #These lines establish the feed-forward part of the network used to choose actions\n",
    "    input_state = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "    predicted_q_values = tf.matmul(input_state, W)\n",
    "    predicted_action = tf.argmax(predicted_q_values, 1)\n",
    "\n",
    "    #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "    real_q_values = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(real_q_values - predicted_q_values))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    update_model = trainer.minimize(loss)\n",
    "    \n",
    "    # start\n",
    "    init = tf.global_variables_initializer()\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for k in tqdm(range(1, number_of_runs)):\n",
    "            current_state = env.reset()\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "            current_action = a[0]\n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, 0, env)\n",
    "\n",
    "            done = False\n",
    "            while not done: \n",
    "                a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "\n",
    "                new_state, reward, done, prob = env.step(current_action)\n",
    "\n",
    "                a, q_table_new = sess.run([predicted_action, predicted_q_values], \n",
    "                                          feed_dict={input_state:[one_hot_encoding(new_state, 16)]})\n",
    "\n",
    "                if use_e_greedy_policy_decay:\n",
    "                    new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "                else:\n",
    "                    new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "\n",
    "\n",
    "                # make stimated values real\n",
    "                q_table[0, current_action] = reward + gama* q_table_new[0][new_action]\n",
    "\n",
    "                _,W1 = sess.run([update_model, W],\n",
    "                                    feed_dict={input_state:[one_hot_encoding(current_state, 16)], real_q_values:q_table})\n",
    "\n",
    "                current_state = new_state\n",
    "                current_action = new_action\n",
    "\n",
    "            # trace log   \n",
    "            if reward == 1:\n",
    "                won_count += 1\n",
    "            if use_trace_of_learning and k%1000 == 0:\n",
    "                trace_of_won.append(won_count)\n",
    "                won_count = 0\n",
    "        \n",
    "        q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for current_state in range(16):\n",
    "            a, q_table = sess.run([predicted_action, predicted_q_values], \n",
    "                                      feed_dict={input_state:[one_hot_encoding(current_state, 16)]})\n",
    "\n",
    "            q_table_final[current_state] = q_table\n",
    "        print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based on numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_table_approximation(W, state, env):\n",
    "    # forward propagetion\n",
    "    qa1 = np.dot(W, prepare_x(state, 0))\n",
    "    qa2 = np.dot(W, prepare_x(state, 1))\n",
    "    qa3 = np.dot(W, prepare_x(state, 2))\n",
    "    qa4 = np.dot(W, prepare_x(state, 3))\n",
    "    \n",
    "    q_table = np.array([[qa1, qa2, qa3, qa4]])\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def evaluate_nn_policy(env, W, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            q_table = q_table_approximation(W, state, env)\n",
    "            action = greedy_policy(q_table, 0)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes\n",
    "\n",
    "def prepare_x(state, action):\n",
    "    one_hot_state = one_hot_encoding(state, 16)\n",
    "    one_hot_action = one_hot_encoding(action, 4)\n",
    "    X = np.hstack((one_hot_state, one_hot_action))\n",
    "    \n",
    "    return X\n",
    "\n",
    "def vf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # learning\n",
    "        # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            Gt = trajectory_total_discounted_rewards[step]\n",
    "            q_table = q_table_approximation(W, step_state, env)\n",
    "            estimated_Gt = q_table[0][step_action]\n",
    "            \n",
    "            error = Gt - estimated_Gt\n",
    "            X = prepare_x(step_state, step_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "    \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won\n",
    "    \n",
    "        \n",
    "def semi_gradient_sarsa_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # state, actions\n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    #W = np.random.random(H) / np.sqrt(H)\n",
    "    # W = np.zeros(env.observation_space.n + env.action_space.n)\n",
    "    W_start = np.copy(W)\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        q_table = q_table_approximation(W, current_state, env)\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if done and reward==0:\n",
    "                reward = -10\n",
    "            if done and reward==1:\n",
    "                reward = 10\n",
    "            \n",
    "            if done:\n",
    "                error = (reward - q_table[0][current_action])\n",
    "                X = prepare_x(current_state, current_action)\n",
    "                grad = error * X # backpropagate error\n",
    "                W += learning_rate * grad\n",
    "                continue\n",
    "            \n",
    "            q_table_new = q_table_approximation(W, new_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "            \n",
    "            # learning\n",
    "            error = (reward + gama* q_table_new[0][new_action] - q_table[0][current_action])\n",
    "            X = prepare_x(current_state, current_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 10:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "    \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [03:01<00:00, 55.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.58831608e-01 3.98795724e-01 4.09490466e-01 3.90214682e-01]\n",
      " [2.07987949e-01 2.43810102e-01 2.64999241e-01 4.54139203e-01]\n",
      " [2.55580544e-01 2.11779639e-01 2.45777413e-01 4.30581897e-01]\n",
      " [1.61982223e-01 1.52494252e-01 9.00737569e-02 3.98743898e-01]\n",
      " [4.71385568e-01 3.22831810e-01 3.41931403e-01 2.83047676e-01]\n",
      " [8.10499862e-03 9.91986971e-03 3.87874478e-03 9.90386307e-03]\n",
      " [3.09765935e-01 8.82254317e-02 8.73714387e-02 4.30524237e-02]\n",
      " [1.08926295e-04 3.70651484e-04 2.36719847e-03 4.65553254e-03]\n",
      " [3.12165916e-01 3.73096406e-01 2.74822205e-01 4.85897660e-01]\n",
      " [2.93524414e-01 5.42905986e-01 3.82126987e-01 2.17102587e-01]\n",
      " [6.94001436e-01 2.07103506e-01 2.62417704e-01 1.75779343e-01]\n",
      " [3.11495067e-04 2.90249591e-03 1.64100283e-03 6.81203615e-04]\n",
      " [8.01341608e-03 8.62472050e-04 4.91964445e-03 2.38498324e-03]\n",
      " [3.70978653e-01 3.07541162e-01 6.60973787e-01 2.79178500e-01]\n",
      " [6.51106358e-01 9.47257161e-01 6.20488763e-01 6.24222577e-01]\n",
      " [4.74867458e-03 1.19681354e-03 8.00783467e-03 6.96068397e-03]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1133c4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1133efd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 0.0\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "y = []\n",
    "number_of_try = 1\n",
    "number_of_runs = 10000\n",
    "q_tables = []\n",
    "\n",
    "for i in range(number_of_try):\n",
    "    # train\n",
    "#     W, trace_of_learning, trace_of_won = semi_gradient_sarsa_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                       use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "\n",
    "    \n",
    "    W, trace_of_learning, trace_of_won = vf_tf_sarsa_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "                                                                use_e_greedy_policy_decay=True)\n",
    "    \n",
    "#     W, trace_of_learning, trace_of_won = vf_tf_monte_carlo_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "#                                                                 use_e_greedy_policy_decay=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "    result = 0\n",
    "    q_table = []\n",
    "    \n",
    "#     q_table, trace_of_learning, trace_of_won = sarsa_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "\n",
    "#     q_table, trace_of_learning, trace_of_won = q_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=False, use_trace_of_learning=True)\n",
    "    sleep(0.1)\n",
    "    # evaluate\n",
    "    #result = evaluate_policy(env, q_table, 100)\n",
    "\n",
    "    # data for charts\n",
    "    x.append(i)\n",
    "    y.append(result)\n",
    "    q_tables.append(q_table)\n",
    "\n",
    "    # show learning\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(trace_of_learning)\n",
    "    plt.title('Learning process')\n",
    "    plt.ylabel('Results')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(trace_of_won)\n",
    "    plt.xlabel('tries (each step is 1000)')\n",
    "    plt.ylabel('Won games over time')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# total analitics    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set(xlabel='try', ylabel='results', title='AVG return from Policy')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(str(number_of_runs) + '_' + str(number_of_try) + '_plot.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean', np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_tables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mclean_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mclean_q_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36mget_clean_q_table\u001b[0;34m(q_table)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_clean_q_table(q_table):\n",
    "    clean = np.zeros(q_table.shape)\n",
    "    for state in range(q_table.shape[0]):\n",
    "        max_action = np.argmax(q_table[state,:])\n",
    "        clean[state][max_action] = 1\n",
    "    return clean\n",
    "\n",
    "clean_q_table = [] \n",
    "total_sum = np.zeros((16,4))\n",
    "for q_table in q_tables:\n",
    "    clean_table = get_clean_q_table(q_table)\n",
    "    clean_q_table.append(clean_table)\n",
    "    total_sum = np.add(total_sum, clean_table)\n",
    "    \n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# for n in range(number_of_try):\n",
    "#     xs = np.arange(16*4)\n",
    "#     ys = clean_q_table[n].flatten()\n",
    "#     ax.bar(xs, ys, zs=n, zdir='y')\n",
    "# ax.set_xlabel('state-action')\n",
    "# ax.set_ylabel('tries')\n",
    "# ax.set_zlabel('value: 0 or 1')\n",
    "# plt.show()\n",
    "\n",
    "print(total_sum/number_of_try)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
