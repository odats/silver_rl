{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Elementary Reinforcement Learning from RL Course by David Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odats/openai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "s0 = env.reset()\n",
    "env.render()\n",
    "# when you make action there is 33% chance to get to another space\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(q_table, state, env, epsilon = 0.25):\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()     \n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "    return action\n",
    "\n",
    "def e_greedy_policy_decay(q_table, state, k, number_of_runs, env):\n",
    "    epsilon = 1 - (k/number_of_runs)\n",
    "    \n",
    "    actions_count = env.action_space.n\n",
    "    act_greedy = epsilon / actions_count + (1 - epsilon)\n",
    "    \n",
    "    if random.random() < act_greedy:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "\n",
    "def ucb_policy(q_table, state, q_table_n, k, number_of_runs, env):\n",
    "    action = np.argmax(np.add(q_table[state,:], 2*(np.log(k) / q_table_n[state,:])))\n",
    "    \n",
    "    return action\n",
    "\n",
    "def random_policy(q_table, state, env):\n",
    "    action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def greedy_policy(q_table, state):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, q_table, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    state_values = np.zeros(env.observation_space.n)\n",
    "    n_of_s = np.zeros(env.observation_space.n)\n",
    "\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    #q_table = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "    q_table_n = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # fill action states values\n",
    "        q_table_n_first = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            \n",
    "            # MC first occurrence\n",
    "            if MC_first_occurrence:\n",
    "                if q_table_n_first[step_state][step_action] != 0:\n",
    "                    continue\n",
    "                q_table_n_first[step_state][step_action] = 1\n",
    "            \n",
    "            q_table_n[step_state][step_action] += 1\n",
    "            n = q_table_n[step_state][step_action]\n",
    "\n",
    "            current_value = q_table[step_state][step_action]\n",
    "            alfa = (1/n)\n",
    "            new_value = current_value + alfa * (trajectory_total_discounted_rewards[step] - current_value)\n",
    "            q_table[step_state][step_action] = new_value\n",
    "\n",
    "        # fill states values\n",
    "        for step, state in enumerate(trajectory_states):\n",
    "            n_of_s[state] += 1 \n",
    "            state_values[state] = state_values[state] + (1/n_of_s[state]) * (trajectory_total_discounted_rewards[step]-state_values[state])\n",
    "            \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table, new_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table, new_state, env)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][new_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning. SARSA off model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "                    \n",
    "            target_action = greedy_policy(q_table, new_state)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][target_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_vf_tf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    #These lines establish the feed-forward part of the network used to choose actions\n",
    "    inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "    W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "    Qout = tf.matmul(inputs1,W)\n",
    "    predict = tf.argmax(Qout,1)\n",
    "\n",
    "    #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "    nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    updateModel = trainer.minimize(loss)\n",
    "    \n",
    "    # start\n",
    "    init = tf.global_variables_initializer()\n",
    "    e = 0.1\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for k in tqdm(range(1, number_of_runs)):\n",
    "            trajectory_states = []\n",
    "            trajectory_actions = []\n",
    "            trajectory_rewards = []\n",
    "            trajectory_total_discounted_rewards = []\n",
    "\n",
    "            current_state = env.reset()\n",
    "            done = False\n",
    "            while not done: \n",
    "                a, q_table = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[current_state:current_state+1]})\n",
    "                action = a[0]\n",
    "                if np.random.rand(1) < e:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "                trajectory_states.append(current_state)\n",
    "                trajectory_actions.append(action)\n",
    "                trajectory_rewards.append(reward)\n",
    "\n",
    "                current_state = new_state\n",
    "\n",
    "            for idx_state, state in enumerate(trajectory_states):\n",
    "                total_discounted_reward = 0\n",
    "                for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                    total_discounted_reward += reward * (gama**idx_reward)\n",
    "                trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "\n",
    "            # learning\n",
    "            # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "            for step in range(len(trajectory_states)):\n",
    "                step_state = trajectory_states[step]\n",
    "                step_action = trajectory_actions[step]\n",
    "                Gt = trajectory_total_discounted_rewards[step]\n",
    "                estimated_q_table = sess.run(Qout, feed_dict={inputs1:np.identity(16)[step_state:step_state+1]})\n",
    "                estimated_q_table[0, step_action] = Gt\n",
    "\n",
    "                #Train our network using target and predicted Q values\n",
    "                _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[step_state:step_state+1],nextQ:estimated_q_table})\n",
    "\n",
    "            e = 1./((i/50) + 10)\n",
    "            # trace log   \n",
    "            if reward == 1:\n",
    "                won_count += 1\n",
    "            if use_trace_of_learning and k%1000 == 0:\n",
    "                trace_of_won.append(won_count)\n",
    "                won_count = 0\n",
    "        \n",
    "#         q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#         for state in range(16):\n",
    "#             a, q_table = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[current_state:state+1]})\n",
    "#             q_table_final[state] = q_table\n",
    "#         print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1\n",
    "\n",
    "def q_table_approximation(W, state, env):\n",
    "    # forward propagetion\n",
    "    qa1 = np.dot(W, prepare_x(state, 0))\n",
    "    qa2 = np.dot(W, prepare_x(state, 1))\n",
    "    qa3 = np.dot(W, prepare_x(state, 2))\n",
    "    qa4 = np.dot(W, prepare_x(state, 3))\n",
    "    \n",
    "    q_table = np.array([[qa1, qa2, qa3, qa4]])\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def evaluate_nn_policy(env, W, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            q_table = q_table_approximation(W, state, env)\n",
    "            action = greedy_policy(q_table, 0)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes\n",
    "\n",
    "def one_hot_encoding(item, size=1):\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[item]=1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def prepare_x(state, action):\n",
    "    one_hot_state = one_hot_encoding(state, 16)\n",
    "    one_hot_action = one_hot_encoding(action, 4)\n",
    "    X = np.hstack((one_hot_state, one_hot_action))\n",
    "    \n",
    "    return X\n",
    "\n",
    "def vf_monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # learning\n",
    "        # w ← w + α[Gt − vˆ(St,w)] ∇vˆ(St,w)\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            Gt = trajectory_total_discounted_rewards[step]\n",
    "            q_table = q_table_approximation(W, step_state, env)\n",
    "            estimated_Gt = q_table[0][step_action]\n",
    "            \n",
    "            error = Gt - estimated_Gt\n",
    "            X = prepare_x(step_state, step_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "    \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won\n",
    "    \n",
    "        \n",
    "def semi_gradient_sarsa_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # state, actions\n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    W = np.random.randn(H) / np.sqrt(H)\n",
    "    #W = np.random.random(H) / np.sqrt(H)\n",
    "    # W = np.zeros(env.observation_space.n + env.action_space.n)\n",
    "    W_start = np.copy(W)\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        q_table = q_table_approximation(W, current_state, env)\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            q_table = q_table_approximation(W, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if done and reward==0:\n",
    "                reward = -10\n",
    "            if done and reward==1:\n",
    "                reward = 10\n",
    "            \n",
    "            if done:\n",
    "                error = (reward - q_table[0][current_action])\n",
    "                X = prepare_x(current_state, current_action)\n",
    "                grad = error * X # backpropagate error\n",
    "                W += learning_rate * grad\n",
    "                continue\n",
    "            \n",
    "            q_table_new = q_table_approximation(W, new_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "            \n",
    "            # learning\n",
    "            error = (reward + gama* q_table_new[0][new_action] - q_table[0][current_action])\n",
    "            X = prepare_x(current_state, current_action)\n",
    "            grad = error * X # backpropagate error\n",
    "            W += learning_rate * grad\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 10:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "    \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [01:59<00:00, 83.45it/s] \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113283e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAGFRJREFUeJzt3Xu0nXV95/H3R6LxWkNAIxA0gDgOOFY7Z8BLO+uM3LVcluIUtRA7Wqa1rrE6dsSlSyiFFpmpto62NUU0Qi0qLTQtOAjoaa21ykVcQqsm3JpwFQhIwHDzO3/sJ7o5nCQ7Ob99dnbyfq2113kuv/083985yf6c3/N79j6pKiRJmq0njboASdL2wUCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKtI1J8uokK5OsS3LMqOsZVJJTkpzbLT+/q3+nUdeluWOgaKSSTCVZm2R+37aTkvzDDG13TfJwkpd067sl+fMkt3YvXjck+UySFzes76YkB7c63oBOBT5eVc+sqgvn+Nwb+vzj7nt6R/c9feaWHKOq/q2r/7Fh1altj4GikUmyBPgloICj+nadC7wqyV7TnnIc8N2qujbJLsA/AU/vjvEs4BeAvwcOGfD882ZT/xDP8QLguo0cL0nm4v/tkVX1THrf0wngg3NwTo05A0WjdALwz8BngKUbNlbVGuArwPEztP9st/xu4EfA8VV1ffXcW1Wfrqr/O9PJkkwmWZPkfUluBz7dbf/lJNckuTfJPyV5abf9HOD5wN92v63/rw3HmHbcn45iuss+5yc5N8mPgLd2276Q5LNJ7k9yXZKJjdR4PbB33znnd6O405N8HXgQ2DvJ7klWJLknyaokv953jFOSfLGr4f4k303yoiTvT3JnktVJDt3Mz2bDz+IW4EvAhlHhRs87rR9LktSGQE2yMMmnu9Hk2iQXdtuvTXJk3/OenOSuJC8fpD5tWwwUjdIJwF90j8OSLOrbt5y+QEny74CXAZ/rNh0MXFBVP9nCcz4PWEhvFHBi98J1NvDfgV2ATwIrksyvquOBf6P7bb2qzhzwHEcD5wMLur5BbwR2XrdtBfDxmZ5YVftMO+dD3a7jgRPpjcRu7o61BtgdOBb4/SSv6TvUkcA5wM7At4FL6P1/34PeJbVPDtKRJHsCr+2OwQDn3Zhz6I0m9weeC3y02/5Z4Ff72r0WuK2qvo3GjoGikUjyi/Re1L9QVVcB1wNv7mtyAbAoyau69ROAL1XVD7v1XYHb+453VDfCuD/Jlzdx6p8AJ1fVQ1X1Y3ov0p+sqm9W1WNVtRx4CHjFLLr3jaq6sKp+0p0D4B+r6uJuTuEc4Oe38JifqarrqupReqH4auB9VbW+qq4BzqL3Pdrga1V1Sdf+i8BzgDOq6hF6obAkyYJNnO/CJPcC/0jvMuLvd+GyufM+QZLdgCOA36iqtVX1SFX9fbf7XOC1SX6uWz+e3vdHY8hA0agsBb5cVXd165/j8Ze9HqT3QnhCkgBv4WeXuwDuBnbra7+iqhbQuxT2lE2c94dVtb5v/QXA/+zC6N7uRXRPer+Bb63VM2y7vW/5QeCpWzi/0n/M3YF7qur+vm030xt9bHBH3/KPgbv6Jsg3hNymJtqPqaoFVfWCqnpHF4yDnHcme3bPWzt9R1XdCnwdeEMXcEfws1GdxszQJyWl6ZI8DfivwE7dXAbAfGBBkp+vqu9025YDFwJ/Te9Sz9/2HeZy4Jgkv7uFl72mf7z2auD0qjp9wPYP0Lt0s6EvO9H77X9Tz2mh/5i3AguTPKvvxf35wC1DOG+/rT3v6u55C6rq3hn2LwfeTu/16BvdvI3GkCMUjcIxwGPAfvTmRV4G/Hvga0y7bAPcCywDzquqh/v2fYTe/MA5Sfbp7n56VnesLfHnwG8kObA7xjOSvK47FvR+09+7r/0P6I0uXpfkyfTufprPHKqq1fTucPuDJE/tbiJ4G73LR9vceavqNnoT+3+SZOdu4v0/9zW5kN7dZO/i8aNQjRkDRaOwFPh0916F2zc86E1Uv2XDpaDq/bGez9K7LPW4F5ruUtkrgPX0rvPfD1xDbyTzm4MWUlVXAr/enXstsAp4a1+TPwA+2F0Oe29V3Qe8g97cwS30RiyPu+trjrwJWEJv1HABvXmhy7bh8x4PPAJ8D7gT+O0NO7rLaX8F7EVvNKoxFf/AlqRRS/Ih4EVV9aubbaxtlnMokkYqyUJ6l86mv+9IY8ZLXpJGpntj5Gp6t4Q/4eN2NF685CVJasIRiiSpiR1qDmXXXXetJUuWjLqMLfLAAw/wjGc8Y9RlzCn7vGOwz+Pjqquuuquqpr/f6gl2qEBZsmQJV1555ajL2CJTU1NMTk6Ouow5ZZ93DPZ5fCS5eZB2XvKSJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDUx0kBJcniS7ydZleSkGfbPT/L5bv83kyyZtv/5SdYlee9c1SxJmtnIAiXJTsAngCOA/YA3JdlvWrO3AWur6oXAR4EPT9v/EeBLw65VkrR5oxyhHACsqqobquph4Dzg6GltjgaWd8vnAwclCUCSY4AbgevmqF5J0ibMG+G59wBW962vAQ7cWJuqejTJfcAuSdYD7wMOATZ5uSvJicCJAIsWLWJqaqpJ8XNl3bp1Y1fzbNnnHYN93v6MMlBm4xTgo1W1rhuwbFRVLQOWAUxMTNTk5OTQi2tpamqKcat5tuzzjsE+b39GGSi3AHv2rS/uts3UZk2SecCzgbvpjWSOTXImsAD4SZL1VfXx4ZctSZrJKAPlCmDfJHvRC47jgDdPa7MCWAp8AzgW+EpVFfBLGxokOQVYZ5hI0miNLFC6OZF3ApcAOwFnV9V1SU4FrqyqFcCngHOSrALuoRc6kqRt0EjnUKrqYuDiads+1Le8HnjjZo5xylCKkyRtEd8pL0lqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEyMNlCSHJ/l+klVJTpph//wkn+/2fzPJkm77IUmuSvLd7utr5rp2SdLjjSxQkuwEfAI4AtgPeFOS/aY1exuwtqpeCHwU+HC3/S7gyKr6D8BS4Jy5qVqStDGjHKEcAKyqqhuq6mHgPODoaW2OBpZ3y+cDByVJVX27qm7ttl8HPC3J/DmpWpI0o1EGyh7A6r71Nd22GdtU1aPAfcAu09q8Abi6qh4aUp2SpAHMG3UBs5Fkf3qXwQ7dRJsTgRMBFi1axNTU1NwU18i6devGrubZss87Bvu8/RlloNwC7Nm3vrjbNlObNUnmAc8G7gZIshi4ADihqq7f2EmqahmwDGBiYqImJydb1T8npqamGLeaZ8s+7xjs8/ZnlJe8rgD2TbJXkqcAxwErprVZQW/SHeBY4CtVVUkWABcBJ1XV1+esYknSRo0sULo5kXcClwD/Cnyhqq5LcmqSo7pmnwJ2SbIKeA+w4dbidwIvBD6U5Jru8dw57oIkqc9I51Cq6mLg4mnbPtS3vB544wzPOw04begFSpIG5jvlJUlNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKkJA0WS1ISBIklqwkCRJDVhoEiSmjBQJElNGCiSpCYMFElSEwaKJKmJgQIlyTOSPKlbflGSo5I8ebilSZLGyaAjlH8AnppkD+DLwPHAZ4ZVlCRp/AwaKKmqB4HXA39SVW8E9h9eWZKkcTNwoCR5JfAW4KJu207DKUmSNI4GDZR3Ae8HLqiq65LsDXx1eGVJksbNvAHbLaqqozasVNUNSb42pJokSWNo0BHK+wfcJknaQW1yhJLkCOC1wB5JPta36+eAR4dZmCRpvGzuktetwFXAUd3XDe4H3j2soiRJ42eTgVJV3wG+k+TcqnJEIknaqM1d8vouUN3yE/ZX1UuHU5Ykadxs7pLXLw/z5EkOB/6Y3ntazqqqM6btnw98FviPwN3Ar1TVTd2+9wNvAx4D/kdVXTLMWiVJm7a5S143D+vESXYCPgEcAqwBrkiyoqr+pa/Z24C1VfXCJMcBHwZ+Jcl+wHH03q2/O3BZkhdV1WPDqleStGmDfjjk/Ul+1D3WJ3ksyY9mee4DgFVVdUNVPQycBxw9rc3RwPJu+XzgoPSuvR0NnFdVD1XVjcCq7niSpBEZ6I2NVfWsDct9L+ivmOW59wBW962vAQ7cWJuqejTJfcAu3fZ/nvbcPWY6SZITgRMBFi1axNTU1CzLnlvr1q0bu5pnyz7vGOzz9mfQd8r/VFUVcGGSk4GT2pfUVlUtA5YBTExM1OTk5GgL2kJTU1OMW82zZZ93DPZ5+zNQoCR5fd/qk4AJYP0sz30LsGff+uJu20xt1iSZBzyb3uT8IM+VJM2hQUcoR/YtPwrcxBPnO7bUFcC+SfaiFwbHAW+e1mYFsBT4BnAs8JWqqiQrgM8l+Qi9Sfl9gW/Nsh5J0iwMOofya61P3M2JvBO4hN5tw2d3n2R8KnBlVa0APgWck2QVcA+90KFr9wXgX+gF3G95h5ckjdagl7zOBE4Dfgz8P+ClwLur6tzZnLyqLgYunrbtQ33L64E3buS5pwOnz+b8kqR2Bv204UOr6kf03uh4E/BC4HeGVZQkafwMGigbRjKvA75YVfcNqR5J0pgadFL+75J8j94lr99M8hxmf5eXJGk7MtAIpapOAl4FTFTVI8CDzP4uL0nSdmTQj155OvAO4E+7TbvTey+KJEnA4HMonwYepjdKgd77Rk4bSkWSpLE0aKDsU1VnAo8AVNWDwBP/QIokaYc1aKA8nORp/OyPbe0DPDS0qiRJY2ezd3l1ny78Z/Te0Lhnkr8AXg28dbilSZLGyWYDpfvsrN8BJul9ZH2Ad1XVXUOuTZI0RgZ9H8rVwN5VddEwi5Ekja9BA+VA4C1JbgYeoDdKqap66dAqkySNlUED5bChViFJGnuDfnz9zcMuRJI03ga9bViSpE0yUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1MZJASbIwyaVJVnZfd95Iu6Vdm5VJlnbbnp7koiTfS3JdkjPmtnpJ0kxGNUI5Cbi8qvYFLu/WHyfJQuBken/P/gDg5L7g+T9V9WLg5cCrkxwxN2VLkjZmVIFyNLC8W14OHDNDm8OAS6vqnqpaC1wKHF5VD1bVVwGq6mHgamDxHNQsSdqEUQXKoqq6rVu+HVg0Q5s9gNV962u6bT+VZAFwJL1RjiRphOYN68BJLgOeN8OuD/SvVFUlqa04/jzgL4GPVdUNm2h3InAiwKJFi5iamtrSU43UunXrxq7m2bLPOwb7vP0ZWqBU1cEb25fkjiS7VdVtSXYD7pyh2S3AZN/6YmCqb30ZsLKq/mgzdSzr2jIxMVGTk5Obar7NmZqaYtxqni37vGOwz9ufUV3yWgEs7ZaXAn8zQ5tLgEOT7NxNxh/abSPJacCzgd+eg1olSQMYVaCcARySZCVwcLdOkokkZwFU1T3A7wFXdI9Tq+qeJIvpXTbbD7g6yTVJ3j6KTkiSfmZol7w2paruBg6aYfuVwNv71s8Gzp7WZg2QYdcoSdoyvlNektSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMjCZQkC5NcmmRl93XnjbRb2rVZmWTpDPtXJLl2+BVLkjZnVCOUk4DLq2pf4PJu/XGSLAROBg4EDgBO7g+eJK8H1s1NuZKkzRlVoBwNLO+WlwPHzNDmMODSqrqnqtYClwKHAyR5JvAe4LQ5qFWSNIB5Izrvoqq6rVu+HVg0Q5s9gNV962u6bQC/B/wh8ODmTpTkROBEgEWLFjE1NbWVJY/GunXrxq7m2bLPOwb7vP0ZWqAkuQx43gy7PtC/UlWVpLbguC8D9qmqdydZsrn2VbUMWAYwMTFRk5OTg55qmzA1NcW41Txb9nnHYJ+3P0MLlKo6eGP7ktyRZLequi3JbsCdMzS7BZjsW18MTAGvBCaS3ESv/ucmmaqqSSRJIzOqOZQVwIa7tpYCfzNDm0uAQ5Ps3E3GHwpcUlV/WlW7V9US4BeBHxgmkjR6owqUM4BDkqwEDu7WSTKR5CyAqrqH3lzJFd3j1G6bJGkbNJJJ+aq6Gzhohu1XAm/vWz8bOHsTx7kJeMkQSpQkbSHfKS9JasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktSEgSJJasJAkSQ1YaBIkpowUCRJTRgokqQmDBRJUhMGiiSpCQNFktREqmrUNcyZJD8Ebh51HVtoV+CuURcxx+zzjsE+j48XVNVzNtdohwqUcZTkyqqaGHUdc8k+7xjs8/bHS16SpCYMFElSEwbKtm/ZqAsYAfu8Y7DP2xnnUCRJTThCkSQ1YaBIkpowULYBSRYmuTTJyu7rzhtpt7RrszLJ0hn2r0hy7fArnr3Z9DnJ05NclOR7Sa5LcsbcVr9lkhye5PtJViU5aYb985N8vtv/zSRL+va9v9v+/SSHzWXds7G1fU5ySJKrkny3+/qaua59a8zmZ9ztf36SdUneO1c1D0VV+RjxAzgTOKlbPgn48AxtFgI3dF937pZ37tv/euBzwLWj7s+w+ww8HfgvXZunAF8Djhh1nzbSz52A64G9u1q/A+w3rc07gD/rlo8DPt8t79e1nw/s1R1np1H3ach9fjmwe7f8EuCWUfdnmP3t238+8EXgvaPuz2wejlC2DUcDy7vl5cAxM7Q5DLi0qu6pqrXApcDhAEmeCbwHOG0Oam1lq/tcVQ9W1VcBquph4Gpg8RzUvDUOAFZV1Q1drefR63u//u/F+cBBSdJtP6+qHqqqG4FV3fG2dVvd56r6dlXd2m2/DnhakvlzUvXWm83PmCTHADfS6+9YM1C2DYuq6rZu+XZg0Qxt9gBW962v6bYB/B7wh8CDQ6uwvdn2GYAkC4AjgcuHUWQDm+1Df5uqehS4D9hlwOdui2bT535vAK6uqoeGVGcrW93f7pfB9wG/Owd1Dt28URewo0hyGfC8GXZ9oH+lqirJwPdyJ3kZsE9VvXv6ddlRG1af+44/D/hL4GNVdcPWValtUZL9gQ8Dh466liE7BfhoVa3rBixjzUCZI1V18Mb2JbkjyW5VdVuS3YA7Z2h2CzDZt74YmAJeCUwkuYnez/O5SaaqapIRG2KfN1gGrKyqP2pQ7rDcAuzZt7642zZTmzVdSD4buHvA526LZtNnkiwGLgBOqKrrh1/urM2mvwcCxyY5E1gA/CTJ+qr6+PDLHoJRT+L4KID/zeMnqM+coc1CetdZd+4eNwILp7VZwvhMys+qz/Tmi/4KeNKo+7KZfs6jdzPBXvxswnb/aW1+i8dP2H6hW96fx0/K38B4TMrPps8LuvavH3U/5qK/09qcwphPyo+8AB8FvWvHlwMrgcv6XjQngLP62v03ehOzq4Bfm+E44xQoW91ner8BFvCvwDXd4+2j7tMm+vpa4Af07gT6QLftVOCobvmp9O7wWQV8C9i777kf6J73fbbRO9la9hn4IPBA38/1GuC5o+7PMH/GfccY+0Dxo1ckSU14l5ckqQkDRZLUhIEiSWrCQJEkNWGgSJKaMFCkOZJkQZJ3jLoOaVgMFGnuLKD3qbOP071zWhp7/kOW5s4ZwD5JrgEeAdYDa4EXJzkPuKe6j5FJcjpwZ1X98ciqlbaQb2yU5kj34Z1/V1UvSTIJXAS8pKpu7Pb9dVX9QpIn0fsEgQOq6u4RlSttMUco0uh8q3p/54SquinJ3UleTu+j/L9tmGjcGCjS6Dwwbf0s4K30PvL/7DmvRpolJ+WluXM/8KxN7L+A3l/h/E/AJXNSkdSQIxRpjlTV3Um+nuRa4MfAHdP2P5zkq8C9VfXYSIqUZsFJeWkb0U3GXw28sapWjroeaUt5yUvaBiTZj97fyrjcMNG4coQiSWrCEYokqQkDRZLUhIEiSWrCQJEkNWGgSJKa+P9mQM0i0EclAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1132556d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 0.0\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "y = []\n",
    "number_of_try = 1\n",
    "number_of_runs = 10000\n",
    "q_tables = []\n",
    "\n",
    "for i in range(number_of_try):\n",
    "    # train\n",
    "#     W, trace_of_learning, trace_of_won = semi_gradient_sarsa_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                       use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "\n",
    "    \n",
    "    W, trace_of_learning, trace_of_won = super_vf_tf_monte_carlo_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "                                                                use_e_greedy_policy_decay=True)\n",
    "    \n",
    "#     result = evaluate_nn_policy(env, W, 100)\n",
    "#     q_table = W\n",
    "    result = 0\n",
    "    q_table = []\n",
    "    \n",
    "#     q_table, trace_of_learning, trace_of_won = sarsa_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "\n",
    "#     q_table, trace_of_learning, trace_of_won = q_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=False, use_trace_of_learning=True)\n",
    "    sleep(0.1)\n",
    "    # evaluate\n",
    "    #result = evaluate_policy(env, q_table, 100)\n",
    "\n",
    "    # data for charts\n",
    "    x.append(i)\n",
    "    y.append(result)\n",
    "    q_tables.append(q_table)\n",
    "\n",
    "    # show learning\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(trace_of_learning)\n",
    "    plt.title('Learning process')\n",
    "    plt.ylabel('Results')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(trace_of_won)\n",
    "    plt.xlabel('tries (each step is 1000)')\n",
    "    plt.ylabel('Won games over time')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# total analitics    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set(xlabel='try', ylabel='results', title='AVG return from Policy')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(str(number_of_runs) + '_' + str(number_of_try) + '_plot.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean', np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_tables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mclean_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mclean_q_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c5ae55e8a1ec>\u001b[0m in \u001b[0;36mget_clean_q_table\u001b[0;34m(q_table)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_clean_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_clean_q_table(q_table):\n",
    "    clean = np.zeros(q_table.shape)\n",
    "    for state in range(q_table.shape[0]):\n",
    "        max_action = np.argmax(q_table[state,:])\n",
    "        clean[state][max_action] = 1\n",
    "    return clean\n",
    "\n",
    "clean_q_table = [] \n",
    "total_sum = np.zeros((16,4))\n",
    "for q_table in q_tables:\n",
    "    clean_table = get_clean_q_table(q_table)\n",
    "    clean_q_table.append(clean_table)\n",
    "    total_sum = np.add(total_sum, clean_table)\n",
    "    \n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# for n in range(number_of_try):\n",
    "#     xs = np.arange(16*4)\n",
    "#     ys = clean_q_table[n].flatten()\n",
    "#     ax.bar(xs, ys, zs=n, zdir='y')\n",
    "# ax.set_xlabel('state-action')\n",
    "# ax.set_ylabel('tries')\n",
    "# ax.set_zlabel('value: 0 or 1')\n",
    "# plt.show()\n",
    "\n",
    "print(total_sum/number_of_try)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
