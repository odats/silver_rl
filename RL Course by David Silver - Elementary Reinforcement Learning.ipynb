{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Elementary Reinforcement Learning from RL Course by David Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "s0 = env.reset()\n",
    "env.render()\n",
    "# when you make action there is 33% chance to get to another space\n",
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(q_table, state, env, epsilon = 0.25):\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()     \n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "    return action\n",
    "\n",
    "def e_greedy_policy_decay(q_table, state, k, number_of_runs, env):\n",
    "    epsilon = 1 - (k/number_of_runs)\n",
    "    \n",
    "    actions_count = env.action_space.n\n",
    "    act_greedy = epsilon / actions_count + (1 - epsilon)\n",
    "    \n",
    "    if random.random() < act_greedy:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "\n",
    "def ucb_policy(q_table, state, q_table_n, k, number_of_runs, env):\n",
    "    action = np.argmax(np.add(q_table[state,:], 2*(np.log(k) / q_table_n[state,:])))\n",
    "    \n",
    "    return action\n",
    "\n",
    "def random_policy(q_table, state, env):\n",
    "    action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def greedy_policy(q_table, state):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, q_table, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_train(env, number_of_runs=10000, gama=0.9, use_e_greedy_policy_decay=False,\n",
    "                      MC_first_occurrence=False, use_trace_of_learning=True):\n",
    "    \n",
    "    state_values = np.zeros(env.observation_space.n)\n",
    "    n_of_s = np.zeros(env.observation_space.n)\n",
    "\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    #q_table = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "    q_table_n = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        trajectory_states = []\n",
    "        trajectory_actions = []\n",
    "        trajectory_rewards = []\n",
    "        trajectory_total_discounted_rewards = []\n",
    "\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "                #action = ucb_policy(q_table, current_state, q_table_n, k, number_of_runs, env)\n",
    "            else:\n",
    "                action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "            new_state, reward, done, prob = env.step(action)\n",
    "\n",
    "            trajectory_states.append(current_state)\n",
    "            trajectory_actions.append(action)\n",
    "            trajectory_rewards.append(reward)\n",
    "\n",
    "            current_state = new_state\n",
    "\n",
    "        for idx_state, state in enumerate(trajectory_states):\n",
    "            total_discounted_reward = 0\n",
    "            for idx_reward, reward in enumerate(trajectory_rewards[idx_state:]):\n",
    "                total_discounted_reward += reward * (gama**idx_reward)\n",
    "            trajectory_total_discounted_rewards.append(total_discounted_reward)\n",
    "            \n",
    "        # fill action states values\n",
    "        q_table_n_first = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for step in range(len(trajectory_states)):\n",
    "            step_state = trajectory_states[step]\n",
    "            step_action = trajectory_actions[step]\n",
    "            \n",
    "            # MC first occurrence\n",
    "            if MC_first_occurrence:\n",
    "                if q_table_n_first[step_state][step_action] != 0:\n",
    "                    continue\n",
    "                q_table_n_first[step_state][step_action] = 1\n",
    "            \n",
    "            q_table_n[step_state][step_action] += 1\n",
    "            n = q_table_n[step_state][step_action]\n",
    "\n",
    "            current_value = q_table[step_state][step_action]\n",
    "            alfa = (1/n)\n",
    "            new_value = current_value + alfa * (trajectory_total_discounted_rewards[step] - current_value)\n",
    "            q_table[step_state][step_action] = new_value\n",
    "\n",
    "        # fill states values\n",
    "        for step, state in enumerate(trajectory_states):\n",
    "            n_of_s[state] += 1 \n",
    "            state_values[state] = state_values[state] + (1/n_of_s[state]) * (trajectory_total_discounted_rewards[step]-state_values[state])\n",
    "            \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_train(env, number_of_runs=10000, alfa=0.5, gama=0.9,\n",
    "                use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table, new_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table, new_state, env)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][new_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning. SARSA off model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        while not done: \n",
    "            if use_e_greedy_policy_decay:\n",
    "                current_action = e_greedy_policy_decay(q_table, current_state, k, number_of_runs, env)\n",
    "            else:\n",
    "                current_action = e_greedy_policy(q_table, current_state, env)\n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "                    \n",
    "            target_action = greedy_policy(q_table, new_state)\n",
    "            \n",
    "            q_table[current_state][current_action] += alfa*(reward + gama* q_table[new_state][target_action] - q_table[current_state][current_action])\n",
    "\n",
    "            current_state = new_state\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_policy(env, q_table))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "        \n",
    "    return q_table, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1\n",
    "\n",
    "def q_table_approximation(W, state, env):\n",
    "    # forward propagetion\n",
    "    qa1 = np.dot(W, prepare_x(state, 0))\n",
    "    qa2 = np.dot(W, prepare_x(state, 1))\n",
    "    qa3 = np.dot(W, prepare_x(state, 2))\n",
    "    qa4 = np.dot(W, prepare_x(state, 3))\n",
    "    \n",
    "    q_table = np.array([[qa1, qa2, qa3, qa4]])\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def evaluate_nn_policy(env, W, max_episodes=1000): \n",
    "    tot_reward = 0\n",
    "    for ep in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Reward per episode\n",
    "        while not done:\n",
    "            q_table = q_table_approximation(W, state, env)\n",
    "            action = greedy_policy(q_table, 0)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = new_state\n",
    "            if done:\n",
    "                tot_reward += ep_reward\n",
    "                \n",
    "    return tot_reward / max_episodes\n",
    "\n",
    "def one_hot_encoding(item, size=1):\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[item]=1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def prepare_x(state, action):\n",
    "    one_hot_state = one_hot_encoding(state, 16)\n",
    "    one_hot_action = one_hot_encoding(action, 4)\n",
    "    X = np.hstack((one_hot_state, one_hot_action))\n",
    "    \n",
    "    return X\n",
    "        \n",
    "def dqn_learning_train(env, number_of_runs=10000, alfa=0.5, gama=0.9, \n",
    "                     use_e_greedy_policy_decay=False, use_trace_of_learning=True):\n",
    "    \n",
    "    # state, actions, bias\n",
    "    H = env.observation_space.n + env.action_space.n\n",
    "    #W = np.random.randn(H) / np.sqrt(H)\n",
    "    W = np.random.random(H) / np.sqrt(H)\n",
    "    W_start = np.copy(W)\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    trace_of_learning = []\n",
    "    trace_of_won = []\n",
    "    won_count = 0\n",
    "    for k in tqdm(range(1, number_of_runs)):\n",
    "        current_state = env.reset()\n",
    "        q_table = q_table_approximation(W, current_state, env)\n",
    "        if use_e_greedy_policy_decay:\n",
    "            current_action = e_greedy_policy_decay(q_table, 0, k, number_of_runs, env)\n",
    "        else:\n",
    "            current_action = e_greedy_policy(q_table, 0, env)\n",
    "            \n",
    "        done = False\n",
    "        while not done: \n",
    "            new_state, reward, done, prob = env.step(current_action)\n",
    "            q_table_new = q_table_approximation(W, new_state, env)\n",
    "            if use_e_greedy_policy_decay:\n",
    "                new_action = e_greedy_policy_decay(q_table_new, 0, k, number_of_runs, env)\n",
    "            else:\n",
    "                new_action = e_greedy_policy(q_table_new, 0, env)\n",
    "            \n",
    "            # learning\n",
    "            error = alfa*(reward + gama* q_table_new[0][new_action] - q_table[0][current_action])\n",
    "            X = prepare_x(current_state, current_action)\n",
    "            grad = 2*error * X # backpropagate error\n",
    "            W -= learning_rate * grad\n",
    "\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "      \n",
    "        # trace log   \n",
    "        if reward == 1:\n",
    "            won_count += 1\n",
    "        if use_trace_of_learning and k%1000 == 0:\n",
    "            trace_of_learning.append(evaluate_nn_policy(env, W))\n",
    "            trace_of_won.append(won_count)\n",
    "            won_count = 0\n",
    "    \n",
    "    print(W)\n",
    "    print('W end: ', W - W_start)\n",
    "    \n",
    "    q_table_final = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(16):\n",
    "        for action in range(4):\n",
    "            q_table_final[state][action] = np.dot(W, prepare_x(state, action))      \n",
    "    print(q_table_final)\n",
    "    \n",
    "    return W, trace_of_learning, trace_of_won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 380933/999999 [54:31<1:28:35, 116.45it/s]"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "y = []\n",
    "number_of_try = 1\n",
    "number_of_runs = 1000000\n",
    "q_tables = []\n",
    "\n",
    "for i in range(number_of_try):\n",
    "    # train\n",
    "    W, trace_of_learning, trace_of_won = dqn_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "                                                      use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "    \n",
    "    result = evaluate_nn_policy(env, W, 100)\n",
    "    q_table = W\n",
    "\n",
    "    \n",
    "#     q_table, trace_of_learning, trace_of_won = monte_carlo_train(env, number_of_runs=number_of_runs, gama=0.99,\n",
    "#                                                                 use_e_greedy_policy_decay=True)\n",
    "#     q_table, trace_of_learning, trace_of_won = sarsa_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=True, use_trace_of_learning=True)\n",
    "\n",
    "#     q_table, trace_of_learning, trace_of_won = q_learning_train(env=env, number_of_runs=number_of_runs, alfa=0.1, gama=0.99,\n",
    "#                                                           use_e_greedy_policy_decay=False, use_trace_of_learning=True)\n",
    "    sleep(0.1)\n",
    "    # evaluate\n",
    "    #result = evaluate_policy(env, q_table, 100)\n",
    "\n",
    "    # data for charts\n",
    "    x.append(i)\n",
    "    y.append(result)\n",
    "    q_tables.append(q_table)\n",
    "\n",
    "    # show learning\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(trace_of_learning)\n",
    "    plt.title('Learning process')\n",
    "    plt.ylabel('Results')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(trace_of_won)\n",
    "    plt.xlabel('tries (each step is 1000)')\n",
    "    plt.ylabel('Won games over time')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# total analitics    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set(xlabel='try', ylabel='results', title='AVG return from Policy')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(str(number_of_runs) + '_' + str(number_of_try) + '_plot.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean', np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_q_table(q_table):\n",
    "    clean = np.zeros(q_table.shape)\n",
    "    for state in range(q_table.shape[0]):\n",
    "        max_action = np.argmax(q_table[state,:])\n",
    "        clean[state][max_action] = 1\n",
    "    return clean\n",
    "\n",
    "clean_q_table = [] \n",
    "total_sum = np.zeros((16,4))\n",
    "for q_table in q_tables:\n",
    "    clean_table = get_clean_q_table(q_table)\n",
    "    clean_q_table.append(clean_table)\n",
    "    total_sum = np.add(total_sum, clean_table)\n",
    "    \n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# for n in range(number_of_try):\n",
    "#     xs = np.arange(16*4)\n",
    "#     ys = clean_q_table[n].flatten()\n",
    "#     ax.bar(xs, ys, zs=n, zdir='y')\n",
    "# ax.set_xlabel('state-action')\n",
    "# ax.set_ylabel('tries')\n",
    "# ax.set_zlabel('value: 0 or 1')\n",
    "# plt.show()\n",
    "\n",
    "print(total_sum/number_of_try)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.random(1 + env.action_space.n + 1)\n",
    "print(np.dot(W,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.zeros(3)\n",
    "aa[2]=1\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([12,2,3])\n",
    "c = 2*a*3\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((np.zeros(3), [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[1.038851771907868, 1.7740609585502969, 0.98608582107882137, 0.90494231115216683]])\n",
    "np.argmax(c[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
